# -*- coding: utf-8 -*-
"""
ä¸»å…¥å£ï¼šæ•´åˆ SFT ä¸ GRPO-RL æ¨¡å—ï¼Œæ”¯æŒ --mode é€‰æ‹©è¿è¡Œæ¨¡å¼ã€‚
æ¨¡å¼è¯´æ˜ï¼š
  - sft: ä»…è¿è¡Œ SFT å¾®è°ƒï¼Œæ¨¡å‹ä¿å­˜åˆ° output/sft_model
  - rl: ä»…è¿è¡Œ GRPO å¼ºåŒ–å­¦ä¹ ï¼ˆä» output/sft_model åŠ è½½åˆå§‹æ¨¡å‹ï¼‰ï¼Œæ¨¡å‹ä¿å­˜åˆ° output/rl_model
  - sft+rl: å…ˆè¿è¡Œ SFT å†è¿è¡Œ GRPOï¼Œæ¨¡å‹åˆ†åˆ«ä¿å­˜åˆ°å¯¹åº”ç›®å½•
"""
import os
import sys
import argparse
import logging
import json
from typing import Optional, List

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from utils.data import load_and_preprocess_data
from utils.sft import create_model_and_trainer
from utils.grpo import GRPOTrainerWrapper, GRPOScriptArguments, train_grpo

# ===== å…¨å±€ç¯å¢ƒé…ç½® =====
def setup_env():
    """ç»Ÿä¸€çš„ç¯å¢ƒå˜é‡è®¾ç½®"""
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

    cache_dir = "/tmp/.cache/huggingface"
    os.makedirs(cache_dir, exist_ok=True)
    os.environ["TRANSFORMERS_CACHE"] = cache_dir
    os.environ["HF_HOME"] = cache_dir

    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

    return cache_dir


def detect_gpu_config():
    """æ£€æµ‹GPUé…ç½®å¹¶è¿”å›ä¼˜åŒ–å‚æ•°"""
    if not torch.cuda.is_available():
        return {
            "num_gpus": 0,
            "device": "cpu",
            "use_qlora": False,
            "batch_size": 1,
            "gradient_accumulation_steps": 8
        }

    num_gpus = torch.cuda.device_count()
    total_memory = sum(torch.cuda.get_device_properties(i).total_memory for i in range(num_gpus)) / (1024**3)  # GB

    logger.info(f"æ£€æµ‹åˆ° {num_gpus} ä¸ªGPUï¼Œæ€»æ˜¾å­˜: {total_memory:.1f}GB")

    # æ ¹æ®GPUæ•°é‡å’Œæ˜¾å­˜è°ƒæ•´é…ç½®
    if num_gpus >= 8 and total_memory >= 200:  # 8x3090é…ç½®
        config = {
            "num_gpus": num_gpus,
            "device": "cuda",
            "use_qlora": True,  # ä½¿ç”¨é‡åŒ–èŠ‚çœæ˜¾å­˜
            "batch_size": 2,    # æ¯ä¸ªGPUçš„batch size
            "gradient_accumulation_steps": 4,
            "model_parallel": True
        }
    elif num_gpus >= 4:
        config = {
            "num_gpus": num_gpus,
            "device": "cuda",
            "use_qlora": True,
            "batch_size": 1,
            "gradient_accumulation_steps": 8,
            "model_parallel": True
        }
    else:
        config = {
            "num_gpus": num_gpus,
            "device": "cuda",
            "use_qlora": True,
            "batch_size": 1,
            "gradient_accumulation_steps": 16,
            "model_parallel": False
        }

    logger.info(f"GPUé…ç½®: {config}")
    return config


def load_jsonl_texts(path: str, max_items: Optional[int] = None) -> List[str]:
    """ä» jsonl æ–‡ä»¶åŠ è½½æ–‡æœ¬åˆ—è¡¨"""
    texts = []
    if not os.path.exists(path):
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return texts
    
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if max_items is not None and len(texts) >= max_items:
                break
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                # å°è¯•ä»å¸¸è§å­—æ®µæå–æ–‡æœ¬
                text = None
                for key in ("text", "input", "prompt", "question", "original"):
                    if key in obj and isinstance(obj[key], str) and obj[key].strip():
                        text = obj[key].strip()
                        break
                if text:
                    texts.append(text)
            except Exception as e:
                logger.debug(f"è§£æè¡Œ {i} å¤±è´¥: {e}")
                texts.append(line)  # é™çº§ï¼šç›´æ¥ä½œä¸ºæ–‡æœ¬
    
    logger.info(f"ä» {path} åŠ è½½ {len(texts)} æ¡æ–‡æœ¬")
    return texts


def run_sft(model_name: str, data_path: str, output_dir: str, cache_dir: str):
    """è¿è¡Œ SFT å¾®è°ƒ"""
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹ SFT å¾®è°ƒæ¨¡å—")
    logger.info("=" * 80)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½ tokenizer
    logger.info(f"ğŸ”¤ åŠ è½½ tokenizer: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        use_fast=False,
        cache_dir=cache_dir
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.warning("âš ï¸  Pad token æœªè®¾ç½®ï¼Œä½¿ç”¨ EOS token")
    
    # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    logger.info(f"ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    train_dataset = load_and_preprocess_data(data_path, tokenizer)
    logger.info(f"âœ… åŠ è½½äº† {len(train_dataset)} æ¡è®­ç»ƒæ ·æœ¬")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("ğŸ§  åˆ›å»º SFT è®­ç»ƒå™¨...")
    trainer = create_model_and_trainer(
        model_name=model_name,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        output_dir=output_dir,
        bf16=torch.cuda.is_bf16_supported()
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("âš¡ å¼€å§‹ SFT è®­ç»ƒ...")
    try:
        train_result = trainer.train()
        logger.info(f"âœ… SFT è®­ç»ƒå®Œæˆ: {train_result}")
    except Exception as e:
        logger.exception(f"âŒ SFT è®­ç»ƒå¤±è´¥: {str(e)}")
        raise
    
    # ä¿å­˜æ¨¡å‹
    logger.info("ğŸ’¾ ä¿å­˜ SFT æœ€ç»ˆæ¨¡å‹...")
    try:
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        logger.info(f"âœ… SFT æ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")
    except Exception as e:
        logger.exception(f"âŒ ä¿å­˜ SFT æ¨¡å‹å¤±è´¥: {str(e)}")
        raise
    
    logger.info("=" * 80)
    logger.info("âœ… SFT å¾®è°ƒå®Œæˆ")
    logger.info("=" * 80)
    return output_dir


def run_rl(sft_model_path: str,
           rl_data_path: str,
           output_dir: str,
           cache_dir: str,
           reward_type: str = "advanced",
           max_items: Optional[int] = None,
           epochs: int = 1,
           batch_size: int = 4,
           learning_rate: float = 1e-5,
           max_new_tokens: int = 64,
           temperature: float = 1.0,
           top_p: float = 0.95,
           use_qlora: bool = False,
           gpu_config: dict = None):
    """è¿è¡Œ GRPO å¼ºåŒ–å­¦ä¹ """
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹ GRPO å¼ºåŒ–å­¦ä¹ æ¨¡å—")
    logger.info("=" * 80)
    
    os.makedirs(output_dir, exist_ok=True)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥ SFT æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(sft_model_path):
        raise FileNotFoundError(f"SFT æ¨¡å‹ä¸å­˜åœ¨: {sft_model_path}ã€‚è¯·å…ˆè¿è¡Œ SFT æ¨¡å—æˆ–æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
    
    logger.info(f"ğŸ“¦ åŠ è½½ SFT æ¨¡å‹: {sft_model_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(sft_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            sft_model_path,
            device_map=device if device == "cuda" else None,
            torch_dtype=torch.bfloat16 if use_qlora else torch.float32,
            trust_remote_code=True,
        )
    except Exception as e:
        logger.exception(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        raise
    
    # åŠ è½½ RL æ•°æ®
    logger.info(f"ğŸ“Š åŠ è½½ RL æ•°æ®: {rl_data_path}")
    if not os.path.exists(rl_data_path):
        logger.error(f"RL æ•°æ®ä¸å­˜åœ¨: {rl_data_path}")
        raise FileNotFoundError(f"RL æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {rl_data_path}")
    
    rl_texts = load_jsonl_texts(rl_data_path, max_items=max_items)
    if not rl_texts:
        logger.error("âŒ æœªèƒ½åŠ è½½ RL æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼")
        raise ValueError("RL æ•°æ®ä¸ºç©º")
    
    logger.info(f"âœ… åŠ è½½äº† {len(rl_texts)} æ¡ RL æ•°æ®")
    
    # åˆ›å»º GRPO è®­ç»ƒå™¨
    logger.info("ğŸ§  åˆ›å»º GRPO è®­ç»ƒå™¨...")
    try:
        # æ ¹æ®GPUé…ç½®è°ƒæ•´batch_size
        effective_batch_size = gpu_config.get("batch_size", batch_size)
        logger.info(f"ä½¿ç”¨batch_size: {effective_batch_size}")

        grpo_trainer = GRPOTrainerWrapper(
            model=model,
            tokenizer=tokenizer,
            reward_type=reward_type,
            device=device,
            lr=learning_rate,
            entropy_coef=0.01,
            kl_coef=0.0,
        )
    except Exception as e:
        logger.exception(f"âŒ åˆ›å»º GRPO è®­ç»ƒå™¨å¤±è´¥: {str(e)}")
        raise
    
    # å¼€å§‹è®­ç»ƒå¾ªç¯
    logger.info(f"âš¡ å¼€å§‹ GRPO è®­ç»ƒ (epochs={epochs}, batch_size={batch_size}, reward_type={reward_type})")
    
    try:
        steps_per_epoch = max(1, len(rl_texts) // batch_size)
        for epoch in range(epochs):
            logger.info(f"Epoch {epoch + 1}/{epochs}")
            
            # æŒ‰ batch è¿­ä»£
            for batch_start in range(0, len(rl_texts), batch_size):
                batch_prompts = rl_texts[batch_start:batch_start + batch_size]
                
                try:
                    stats = grpo_trainer.train_step(
                        prompts=batch_prompts,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p
                    )
                    
                    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                    batch_idx = batch_start // batch_size + 1
                    total_batches = steps_per_epoch
                    logger.info(
                        f"  Batch {batch_idx}/{total_batches} | "
                        f"reward_mean={stats.get('reward_mean', 0):.4f} | "
                        f"reward_max={stats.get('reward_max', 0):.4f} | "
                        f"batch_size={len(batch_prompts)}"
                    )
                except Exception as e:
                    logger.exception(f"âŒ è®­ç»ƒ batch å¤±è´¥ï¼Œè·³è¿‡: {str(e)}")
                    continue
            
            # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ¨¡å‹
            logger.info(f"ğŸ’¾ ä¿å­˜ epoch {epoch + 1} æ¨¡å‹...")
            try:
                epoch_output_dir = os.path.join(output_dir, f"epoch_{epoch + 1}")
                os.makedirs(epoch_output_dir, exist_ok=True)
                grpo_trainer.save(epoch_output_dir)
                logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {epoch_output_dir}")
            except Exception as e:
                logger.exception(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°è¾“å‡ºç›®å½•
        logger.info(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆ GRPO æ¨¡å‹åˆ° {output_dir}...")
        grpo_trainer.save(output_dir)
        logger.info(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
        logger.info(f"ğŸ’¾ ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹...")
        grpo_trainer.save(output_dir)
    except Exception as e:
        logger.exception(f"âŒ GRPO è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise
    
    logger.info("=" * 80)
    logger.info("âœ… GRPO å¼ºåŒ–å­¦ä¹ å®Œæˆ")
    logger.info("=" * 80)
    return output_dir


def parse_args():
    """å‘½ä»¤è¡Œå‚æ•°è§£æ"""
    parser = argparse.ArgumentParser(
        description="åŒ»å­¦è€ƒé¢˜ç”Ÿæˆä¼˜åŒ– - SFT ä¸ GRPO ç»¼åˆè®­ç»ƒæ¡†æ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
    ç¤ºä¾‹ç”¨æ³•ï¼š
    # ä»… SFT å¾®è°ƒ
    python main.py --mode sft
    
    # ä»… GRPO å¼ºåŒ–å­¦ä¹ ï¼ˆä» SFT æ¨¡å‹åŠ è½½ï¼‰
    python main.py --mode rl
    
    # å…ˆ SFT å GRPO
    python main.py --mode sft+rl
    
    # è‡ªå®šä¹‰å‚æ•°
    python main.py --mode rl --reward-type basic --epochs 2 --batch-size 8
            """
        )
    
    # åŸºç¡€å‚æ•°
    parser.add_argument(
        "--mode",
        choices=["sft", "rl", "sft+rl"],
        default="sft",
        help="è¿è¡Œæ¨¡å¼ï¼šsft (ä»…å¾®è°ƒ) / rl (ä»…å¼ºåŒ–å­¦ä¹ ) / sft+rl (å…ˆå¾®è°ƒåå¼ºåŒ–å­¦ä¹ )"
    )
    
    # SFT ç›¸å…³å‚æ•°
    parser.add_argument(
        "--model-name",
        type=str,
        default="Qwen/Qwen2.5-7B",
        help="åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„"
    )
    parser.add_argument(
        "--sft-data",
        type=str,
        default="data/sft_train.jsonl",
        help="SFT è®­ç»ƒæ•°æ®è·¯å¾„"
    )
    parser.add_argument(
        "--sft-output",
        type=str,
        default="output/sft_model",
        help="SFT æ¨¡å‹è¾“å‡ºè·¯å¾„"
    )
    
    # RL ç›¸å…³å‚æ•°
    parser.add_argument(
        "--rl-data",
        type=str,
        default="data/rl_train.jsonl",
        help="RL è®­ç»ƒæ•°æ®è·¯å¾„"
    )
    parser.add_argument(
        "--rl-output",
        type=str,
        default="output/rl_model",
        help="RL æ¨¡å‹è¾“å‡ºè·¯å¾„"
    )
    parser.add_argument(
        "--reward-type",
        choices=["basic", "advanced"],
        default="advanced",
        help="Reward è®¡ç®—æ–¹å¼"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=1,
        help="RL è®­ç»ƒè½®æ•°"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="RL batch size"
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-5,
        help="RL å­¦ä¹ ç‡"
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=64,
        help="ç”Ÿæˆæ—¶çš„æœ€å¤§ token æ•°"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=1.0,
        help="ç”Ÿæˆæ¸©åº¦"
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=0.95,
        help="top-p é‡‡æ ·å‚æ•°"
    )
    parser.add_argument(
        "--max-rl-items",
        type=int,
        default=None,
        help="RL æ•°æ®æœ€å¤§æ¡æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    parser.add_argument(
        "--use-qlora",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨ QLoRA é‡åŒ–"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # è®¾ç½®ç¯å¢ƒ
    cache_dir = setup_env()

    # æ£€æµ‹GPUé…ç½®
    gpu_config = detect_gpu_config()

    logger.info("ğŸ¯ è¿è¡Œæ¨¡å¼: %s", args.mode)
    logger.info("ğŸ“‹ é…ç½®å‚æ•°: %s", vars(args))
    logger.info("ğŸ–¥ï¸  GPUé…ç½®: %s", gpu_config)
    
    try:
        if args.mode == "sft":
            # ä»… SFT
            run_sft(
                model_name=args.model_name,
                data_path=args.sft_data,
                output_dir=args.sft_output,
                cache_dir=cache_dir
            )
        
        elif args.mode == "rl":
            # ä»… RL
            run_rl(
                sft_model_path=args.sft_output,
                rl_data_path=args.rl_data,
                output_dir=args.rl_output,
                cache_dir=cache_dir,
                reward_type=args.reward_type,
                max_items=args.max_rl_items,
                epochs=args.epochs,
                batch_size=args.batch_size,
                learning_rate=args.learning_rate,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                use_qlora=args.use_qlora,
                gpu_config=gpu_config
            )
        
        elif args.mode == "sft+rl":
            # SFT + RL
            logger.info("ğŸ”— æ‰§è¡Œ SFT + RL ç»¼åˆè®­ç»ƒæµç¨‹")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šSFT
            sft_model_dir = run_sft(
                model_name=args.model_name,
                data_path=args.sft_data,
                output_dir=args.sft_output,
                cache_dir=cache_dir
            )
            
            # ç¬¬äºŒé˜¶æ®µï¼šRL
            run_rl(
                sft_model_path=sft_model_dir,
                rl_data_path=args.rl_data,
                output_dir=args.rl_output,
                cache_dir=cache_dir,
                reward_type=args.reward_type,
                max_items=args.max_rl_items,
                epochs=args.epochs,
                batch_size=args.batch_size,
                learning_rate=args.learning_rate,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                use_qlora=args.use_qlora,
                gpu_config=gpu_config
            )
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ æ‰€æœ‰è®­ç»ƒæ¨¡å—æ‰§è¡Œå®Œæˆï¼")
        logger.info("=" * 80)
        return 0
    
    except KeyboardInterrupt:
        logger.warning("âŒ ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        logger.exception("âŒ æ‰§è¡Œå¤±è´¥: %s", str(e))
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
