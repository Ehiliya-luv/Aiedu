# main.py
import os
import logging
import torch
from transformers import AutoTokenizer
from utils.data import load_and_preprocess_data
from utils.sft import create_model_and_trainer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

    
if __name__ == "__main__":
    # ===== å…³é”®ç¯å¢ƒè®¾ç½® =====
    # 1. ä½¿ç”¨å›½å†… HF é•œåƒåŠ é€Ÿä¸‹è½½
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
    
    # 2. è®¾ç½®ç¼“å­˜ç›®å½•
    cache_dir = "/tmp/.cache/huggingface"
    os.makedirs(cache_dir, exist_ok=True)
    os.environ["TRANSFORMERS_CACHE"] = cache_dir
    os.environ["HF_HOME"] = cache_dir
    
    # 3. è®¾ç½® CUDA ç¯å¢ƒå˜é‡
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # æ¨¡å‹å’Œè·¯å¾„é…ç½®
    MODEL_NAME = "Qwen/Qwen2.5-7B"
    DATA_PATH = "data/sft_train.jsonl"
    OUTPUT_DIR = "output/qwen2.5-med-mcq-sft"
    
    logger.info(f"ğŸš€ Starting SFT training with model: {MODEL_NAME}")
    logger.info(f"ğŸ“ Data path: {DATA_PATH}")
    logger.info(f"ğŸ’¾ Output directory: {OUTPUT_DIR}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ– tokenizer
    logger.info("ğŸ”¤ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        use_fast=False,
        cache_dir=cache_dir
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.warning("âš ï¸ Pad token not set. Using EOS token as pad token.")
    
    # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    logger.info("ğŸ“Š Loading and preprocessing data...")
    train_dataset = load_and_preprocess_data(DATA_PATH, tokenizer)
    logger.info(f"âœ… Loaded {len(train_dataset)} training samples")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("ğŸ§  Creating SFT trainer...")
    trainer = create_model_and_trainer(
        model_name=MODEL_NAME,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        output_dir=OUTPUT_DIR,
        bf16=torch.cuda.is_bf16_supported()
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("âš¡ Starting SFT training...")
    try:
        train_result = trainer.train()
        logger.info(f"âœ… Training completed successfully: {train_result}")
    except Exception as e:
        logger.exception(f"âŒ Training failed with error: {str(e)}")
        raise
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("ğŸ’¾ Saving final model...")
    try:
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        logger.info(f"âœ… Training completed. Model saved to {OUTPUT_DIR}")
    except Exception as e:
        logger.exception(f"âŒ Failed to save model: {str(e)}")
        raise
