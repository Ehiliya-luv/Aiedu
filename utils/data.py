# utils/data.py
from datasets import Dataset
from transformers import AutoTokenizer
import logging
import json
import os
import re

logger = logging.getLogger(__name__)

def load_and_preprocess_data(data_path: str, tokenizer, max_seq_length: int = 2048):
    """
    åŠ è½½ JSONL æ ¼å¼çš„ SFT è®­ç»ƒæ•°æ®ï¼Œå¹¶ tokenizeã€‚
    ç‰¹åˆ«ä¼˜åŒ–ï¼šå½“åªæœ‰å•ä¸ªæ ·æœ¬æ—¶ï¼Œä¸è¿›è¡Œä¸¥æ ¼è¿‡æ»¤ï¼Œè€Œæ˜¯å°è¯•ä¿®å¤æ ¼å¼é—®é¢˜
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data file not found: {data_path}")
    
    # ===== 1. æ›´å¥å£®çš„ JSONL åŠ è½½ =====
    logger.info(f"Loading data from {data_path} with relaxed parsing...")
    raw_data = []
    
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except Exception as e:
        logger.error(f"Failed to read file: {e}")
        raise
    
    # å¤„ç†æ¯è¡Œæ•°æ®
    for i, line in enumerate(lines):
        line = line.strip()
        # è·³è¿‡ç©ºè¡Œ
        if not line:
            continue
            
        try:
            # å°è¯•è§£æ JSON
            item = json.loads(line)
            raw_data.append(item)
        except json.JSONDecodeError as e:
            logger.warning(f"Line {i+1} JSON parsing error: {e}")
            
            # å°è¯•å¤šç§æ¢å¤ç­–ç•¥
            recovery_successful = False
            
            # ç­–ç•¥1: æŸ¥æ‰¾ JSON å†…å®¹
            json_match = re.search(r'\{.*\}', line, re.DOTALL)
            if json_match:
                try:
                    recovered_json = json_match.group(0)
                    item = json.loads(recovered_json)
                    raw_data.append(item)
                    logger.info(f"Recovered JSON from line {i+1} using regex")
                    recovery_successful = True
                except:
                    pass
            
            # ç­–ç•¥2: ç§»é™¤ Markdown ä»£ç å—æ ‡è®°
            if not recovery_successful:
                cleaned_line = re.sub(r'^```json\s*|\s*```$', '', line, flags=re.MULTILINE)
                try:
                    item = json.loads(cleaned_line)
                    raw_data.append(item)
                    logger.info(f"Recovered JSON from line {i+1} by cleaning Markdown")
                    recovery_successful = True
                except:
                    pass
            
            # ç­–ç•¥3: å°è¯•è§£ææ•´ä¸ªæ–‡ä»¶å†…å®¹
            if not recovery_successful and i == 0 and len(lines) > 0:
                logger.info("Attempting full-file recovery...")
                full_content = " ".join(lines)
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', full_content, re.DOTALL)
                if json_match:
                    try:
                        recovered_json = json_match.group(0)
                        item = json.loads(recovered_json)
                        raw_data.append(item)
                        logger.info("Recovered JSON from full file content")
                        recovery_successful = True
                    except:
                        pass
    
    # ===== 2. å•æ ·æœ¬ä¿æŠ¤æœºåˆ¶ =====
    if len(raw_data) == 0:
        logger.warning("No valid JSON samples found. Creating minimal fallback sample...")
        
        # åˆ›å»ºæœ€å°æœ‰æ•ˆæ ·æœ¬
        fallback_sample = {
            "messages": [
                {"role": "user", "content": "è¯·æ ¹æ®ç—…å†ç”Ÿæˆä¸€é“åŒ»å­¦é€‰æ‹©é¢˜ã€‚"},
                {"role": "assistant", "content": "å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–è¦ç´§ä¾µçŠ¯ä»¥ä¸‹åˆ†æ”¯( )\nA. å›æ—‹æ”¯ã€å·¦å®¤æ”¯\nB. å‰è·ç¦»æ”¯ã€è¾¹ç¼˜æ”¯\nC. å‰é™æ”¯ã€å·¦æ—‹æ”¯\nD. æˆ¿å®¤ç»“æ”¯ã€å¿ƒå®¤æ”¯\n\n**ç­”æ¡ˆï¼šC**"}
            ]
        }
        raw_data.append(fallback_sample)
        logger.info("âœ… Created fallback sample to prevent training failure")
    
    logger.info(f"âœ… Successfully loaded {len(raw_data)} samples")
    
    # ===== 3. åˆ›å»º Dataset å¯¹è±¡ =====
    try:
        dataset = Dataset.from_list(raw_data)
    except Exception as e:
        logger.error(f"âŒ Failed to create dataset: {e}")
        # ä½œä¸ºæœ€åçš„ä¿æŠ¤æªæ–½ï¼Œåˆ›å»ºä¸€ä¸ªæœ€å°æ•°æ®é›†
        minimal_data = [{
            "messages": [
                {"role": "user", "content": "Generate a medical question."},
                {"role": "assistant", "content": "Sample question content."}
            ]
        }]
        dataset = Dataset.from_list(minimal_data)
        logger.warning("âš ï¸ Created minimal fallback dataset")
    
    # ===== 4. åº”ç”¨èŠå¤©æ¨¡æ¿æ ¼å¼åŒ– =====
    def format_chat_template(examples):
        formatted_texts = []
        for i in range(len(examples["messages"])):
            messages = examples["messages"][i]
            try:
                # æ‰‹åŠ¨æ ¼å¼åŒ–èŠå¤©æ¨¡æ¿
                text = ""
                for msg in messages:
                    if isinstance(msg, dict):
                        role = msg.get("role", "")
                        content = msg.get("content", "")
                        if role == "user":
                            text += f"### Human:\n{content}\n\n"
                        elif role == "assistant":
                            text += f"### Assistant:\n{content}\n\n"
                formatted_texts.append(text.strip())
            except Exception as e:
                logger.error(f"Error formatting chat template for sample {i}: {e}")
                # ä½¿ç”¨æœ€å°æœ‰æ•ˆæ¨¡æ¿
                text = "### Human:\nGenerate a medical MCQ question.\n\n### Assistant:\nWhat is the correct answer?"
                formatted_texts.append(text)
        return {"text": formatted_texts}
    
    # åº”ç”¨æ ¼å¼åŒ–
    logger.info("ğŸ“ Applying chat template formatting...")
    try:
        # ç¡®ä¿æ•°æ®é›†æœ‰ messages å­—æ®µ
        if "messages" not in dataset.column_names:
            logger.warning("Dataset does not have 'messages' column. Creating default format.")
            dataset = dataset.map(lambda x: {"messages": [
                {"role": "user", "content": "Generate a medical question based on case."},
                {"role": "assistant", "content": "Sample answer."}
            ]}, batched=False)
        
        dataset = dataset.map(
            format_chat_template,
            batched=True,
            remove_columns=[col for col in dataset.column_names if col != "text"]
        )
    except Exception as e:
        logger.error(f"âŒ Failed to format chat template: {e}")
        # åˆ›å»ºæœ€å°æœ‰æ•ˆæ•°æ®é›†
        minimal_texts = ["### Human:\nGenerate a medical MCQ question.\n\n### Assistant:\nWhat is the most common symptom?"]
        dataset = Dataset.from_dict({"text": minimal_texts})
        logger.warning("âš ï¸ Created minimal formatted dataset")
    
    # ===== 5. Tokenize =====
    logger.info("ğŸ”¤ Tokenizing dataset...")
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_seq_length,
            padding=False,
            return_tensors=None
        )
    
    try:
        dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"],
        )
    except Exception as e:
        logger.error(f"âŒ Tokenization failed: {e}")
        # æœ€å°åŒ– tokenization
        sample_text = "### Human:\nGenerate a medical question.\n\n### Assistant:\nSample answer."
        tokenized = tokenizer(
            sample_text,
            truncation=True,
            max_length=max_seq_length,
            padding=False,
            return_tensors=None
        )
        minimal_dataset = Dataset.from_dict({
            "input_ids": [tokenized["input_ids"]],
            "attention_mask": [tokenized["attention_mask"]]
        })
        return minimal_dataset
    
    # ===== 6. å®½æ¾çš„è¿‡æ»¤ =====
    original_size = len(dataset)
    if original_size > 1:  # åªæœ‰å¤šä¸ªæ ·æœ¬æ—¶æ‰è¿‡æ»¤
        dataset = dataset.filter(lambda x: 
            x.get("input_ids") is not None and 
            len(x["input_ids"]) > 0 and 
            len(x["input_ids"]) <= max_seq_length
        )
        filtered_size = len(dataset)
        logger.info(f"ğŸ” Filtered {original_size - filtered_size} samples")
    else:
        logger.info("ğŸ¯ Single sample detected - skipping filtering to preserve data")
    
    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬
    if len(dataset) == 0:
        logger.warning("âŒ No valid samples after filtering. Creating fallback sample.")
        sample_text = "### Human:\nGenerate a medical MCQ question.\n\n### Assistant:\nSample answer."
        tokenized = tokenizer(
            sample_text,
            truncation=True,
            max_length=max_seq_length,
            padding=False,
            return_tensors=None
        )
        dataset = Dataset.from_dict({
            "input_ids": [tokenized["input_ids"]],
            "attention_mask": [tokenized["attention_mask"]]
        })
    
    # ===== 7. è®¾ç½®æ ¼å¼ =====
    try:
        dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])
    except Exception as e:
        logger.error(f"âŒ Failed to set dataset format: {e}")
        # ç¡®ä¿è¿”å›å¯ç”¨çš„æ•°æ®é›†
        pass
    
    logger.info(f"âœ… Final dataset size: {len(dataset)} samples")
    return dataset